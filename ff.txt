이번 시간에는 feed forward 뉴럴 네트워크에 대해 살펴보겠습니다.
feed forward 네트워크의 가장 간단한case가 linear_regression입니다.
식으로 이렇게 표현합니다.
여기서 x를 독립변수,
y를 종속변수라고 합니다.
베타원은 기울기, 베타제로는 y 절편입니다.
그런데, 뉴럴 네트워크에서는 같은 내용을 다르게 표현하고, 다른 이름을 사용합니다.
뉴럴 네트워크에서 사용하는 식은 이렇습니다.
이것을 regression 관점에서 이야기하면,
x는 독립변수,
y는 종속변수,
w는 기울기,
b는 y 절편입니다.
하지만, 뉴럴 네트워크에서는
x를 example,
y를 lable,
w는 weight,
b는 bias라고 합니다.

# graphic_1
이번에는 regression식을 뉴럴 네트워크 형태로 표시해보겠습니다.
이렇습니다.
x는 여기로 가고,
y는 여기로 가고,
웨이트와 바이어스는 원 안으로 들어갑니다.
결국 뉴럴 네트워크라고 하는 것은 x와 와이를 선형결합으로 묶어주는 역할을 하는 것입니다.



# graphic_2
이번에는 앞에서 보았던 네트워크의 개념을 확장해보겠습니다.
이 네트워크가 표현하는 수식은 무엇일까요?
이것이 정답입니다.
이번에는 x가 한개가 아니라, 5개가 있는 경우입니다. 
x는 여기에 표시되고,
원 안에는 5개의 w와 1개의 바이어스가 있습니다. 여전히 선형결합입니다.
마직막에 계산되어 나오는 y는 하나입니다.
이렇게 그리면 틀린 그림이 됩니다.
노드에서 나오는 값은 항상 하나입니다.










# Graphic_3
이번에는 더 확장해보겠습니다.


















# matrix multiplication
뉴럴 네크워크는 사실 행렬의 곱셈을 그림으로 잘 표현해 놓은 것이라 생각할 수도 있습니다.
weight matrix의 한 행은 하나의 노드 안에 들어 있는 weight 계수의 값입니다.
웨이트 행렬의 행이 늘어난다는 것은 노드의 갯수가 늘어나는 것과 같습니다. 
웨이트 행렬의 열의 갯수는 들어오는 입력값의 디멘젼과 같아야 합니다.
그렇지 않으면, 곱하기 자체가 성립하지 않습니다. 
행렬의 곱셈은 한 행렬의 로우와 다른 행렬의 칼럼을 이용하여 닷 프라덕트를 계산하는 것과 같습니다. 
모든 로우에서 모든 칼럼으로 닷 프라덕트를 하고, 그것을 잘 정돈해 놓으면, 그것이 행렬의 곱셈입니다.

행렬의 곱셈은 기본적으로 모두 리니어입니다. 따라서 리니어한 계산을 통해서 나온 값을 또 다시 리니어 레이어를 통과시켜도 그 결과는 여전히 리니어입니다.
이를 해결하기 위해서 도입된 것이 액티베이션입니다.
따라서 행렬의 곱을 구하고, 거기에 액티베이션 함수를 적용한 것을 아웃풋으로 내보냅니다.
이 아웃풋이 다른 종류의 레이어의 입력이 될 수도 있고, 또는 다른 리니어 레이어의 입력이 될 수도 있습니다.
트랜스포머에 구현된 피드 포워드 네트워크이 리니어 한번, 렐루 액티베이션, 그리고 다시 리어어 레이어를 통과한 것입니다.
이것은 </lang><lang xml:lang="en-US">English</lang> <lang xml:lang="ko-KR">를 말하는 방법입니다.