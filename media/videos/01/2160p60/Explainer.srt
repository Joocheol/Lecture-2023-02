1
00:00:00,000 --> 00:00:05,973
<lang xml:lang="ko-KR">이번 시간에는 세가지의 서로 다른 어텐션을 코드로 어떻게 구현하는가를 살펴보도록

2
00:00:06,073 --> 00:00:12,500
하겠습니다. TensorFlow code를 기준으로 살펴보지만, pi torch에서도 거의 같은 방법으로 사용됩니다.</lang>

3
00:00:13,100 --> 00:00:14,560
<lang xml:lang="ko-KR">시작해보겠습니다.</lang>

4
00:00:15,150 --> 00:00:21,404
<lang xml:lang="ko-KR">우선 base attention이라는 이름을 갖는 클래스를 정의하겠습니다. 이 클래스의 목적은

5
00:00:21,504 --> 00:00:26,234
세가지의 서로 다른 어텐션에서 공통적으로 사용될 layer들을 미리 정의해 놓는 것입니다.</lang>

6
00:00:26,833 --> 00:00:32,096
<lang xml:lang="ko-KR">TensorFlow의 모든 layer는 tf keras layers layer라는 상위

7
00:00:32,196 --> 00:00:35,949
class로부터 상속을 받아야 합니다. 그냥 default라고 생각하면 됩니다.</lang>

8
00:00:36,533 --> 00:00:43,196
<lang xml:lang="ko-KR">이제 이 class의 initialization 스텝을 정의하는, init 라는 함수를

9
00:00:43,296 --> 00:00:47,197
만들겠습니다. 이 함수는 기본적으로 두가지 종류의 일을 합니다. 첫째는 나의

10
00:00:47,297 --> 00:00:51,865
상위클래스를 init하고, 둘째는 내가 사용할 변수를 지정해 놓는 것입니다.</lang>

11
00:00:52,450 --> 00:00:58,470
<lang xml:lang="ko-KR">이 문장은 init 함수의 첫번째 역할인 나의 상위클래스를 init 하는

12
00:00:58,570 --> 00:01:04,396
것입니다. super는 나의 상위 클래스를 의미합니다. 즉, default라고 설명했던 keras layer를

13
00:01:04,496 --> 00:01:08,574
초기화하는 것입니다. 이 문장은 거의 항상 이렇게만 사용됩니다. </lang>

14
00:01:09,166 --> 00:01:15,066
<lang xml:lang="ko-KR">이번에는 init 함수의 두번째 역할인, 사용될 변수들을 정의하는

15
00:01:15,166 --> 00:01:19,439
과정입니다. 세개의 변수를 정의하겠습니다. 이 세가지 변수만 정의하면 base

16
00:01:19,539 --> 00:01:24,930
attention은 끝입니다. 일단, 이름을 보고 무엇을 만들지 짐작해보기 바랍니다.</lang>

17
00:01:26,516 --> 00:01:33,663
<lang xml:lang="ko-KR">m h a는 Multi head attention을 의미합니다. 다행히도 TensorFlow에는 이 layer가

18
00:01:33,763 --> 00:01:37,840
이미 만들어져 있습니다. 그냥 불러서 m h a라는 변수에 넣어주면 됩니다.</lang>

19
00:01:38,433 --> 00:01:44,938
<lang xml:lang="ko-KR">다음으로는 layer normalization이라는 layer를 부르고 그것을 layernorm에

20
00:01:45,038 --> 00:01:48,283
넣습니다. 이 layer의 구체적인 역할에 대해서는 다시 설명하겠지만,

21
00:01:48,383 --> 00:01:51,629
기본적으로 모형이 더 빠르게 수렴하게 하는 역할을 합니다.</lang>

22
00:01:52,216 --> 00:01:58,720
<lang xml:lang="ko-KR">마지막 layer는 더하기 layer입니다. Transformer 아키텍쳐를 기억해 보면, Add and

23
00:01:58,820 --> 00:02:03,204
Norm 이라는 부분이 있었습니다. 그 부분을 구현하기 위해서 필요한 layer입니다.</lang>

24
00:02:03,800 --> 00:02:08,041
<lang xml:lang="ko-KR">지금까지 만든 모든 layer는 다 keras layer입니다.

25
00:02:08,141 --> 00:02:11,260
따라서 앞에 tf keras layers 라고 붙여주어야 합니다.</lang>

26
00:02:11,850 --> 00:02:17,982
<lang xml:lang="ko-KR">마지막으로 이 부분. keyword arguments 부분이 굉장히 신경쓰일텐데, 이것은

27
00:02:18,082 --> 00:02:21,878
나중에 이 클래스를 부를 때 보내주는 인자를 받아오기 위한 것입니다.</lang>

28
00:02:22,466 --> 00:02:27,094
<lang xml:lang="ko-KR">예를 들어, 이렇게 인자들을 보내면, 이것을 여기에서 받아옵니다.</lang>

29
00:02:27,200 --> 00:02:30,268
<lang xml:lang="ko-KR">그리고는, 여기에서 여기로 들어가게 됩니다.</lang>

30
00:02:31,383 --> 00:02:35,920
<lang xml:lang="ko-KR">이상으로 base attention class를 만들어 보았습니다. 이것을

31
00:02:36,020 --> 00:02:39,035
기반으로 세가지의 서로 다른 attention을 구현할 것입니다.</lang>

32
00:02:42,383 --> 00:02:46,691
<lang xml:lang="ko-KR">이번에는 cross attention 부분을 어떻게 구현하는가를

33
00:02:46,791 --> 00:02:50,491
살펴보겠습니다. 앞에서 만들었던 base attention을 이용할 것입니다.</lang>

34
00:02:51,083 --> 00:02:52,543
<lang xml:lang="ko-KR">시작해보겠습니다.</lang>

35
00:02:53,133 --> 00:02:56,949
<lang xml:lang="ko-KR">이 클래스의 이름은 cross attention입니다. 왜

36
00:02:57,049 --> 00:02:59,513
cross라는 말을 사용하는지 곧 알게 될 것입니다.</lang>

37
00:03:00,100 --> 00:03:05,809
<lang xml:lang="ko-KR">이 클래스는 앞에서 만들었던 base attention을 상속받습니다. 따라서 앞에서 정의했던,

38
00:03:05,909 --> 00:03:10,320
multi head attention, layernorm, add 등을 사용할 수 있습니다. </lang>

39
00:03:10,916 --> 00:03:16,107
<lang xml:lang="ko-KR">TensorFlow에서는 class가 실행할 일을, call 이라는 함수를 이용하여

40
00:03:16,207 --> 00:03:19,864
만듭니다. pi torch에서는 forward 라는 함수 이름을 사용합니다.</lang>

41
00:03:20,450 --> 00:03:24,012
<lang xml:lang="ko-KR">함수를 부를 때, 사용하는 인수는 x 와

42
00:03:24,112 --> 00:03:27,430
context입니다. self는 그냥 원래 적어주는 것입니다. </lang>

43
00:03:28,016 --> 00:03:32,105
<lang xml:lang="ko-KR">쿼리, 키, 밸류라는 말이 나옵니다. 쿼리에는

44
00:03:32,205 --> 00:03:35,596
x를 넣어주고, 키와 밸류에는 context를 넣어줍니다. </lang>

45
00:03:36,183 --> 00:03:40,163
<lang xml:lang="ko-KR">이것을 트랜스포머 아키텍쳐를 보면서 설명해보겠습니다.</lang>

46
00:03:40,750 --> 00:03:43,650
<lang xml:lang="ko-KR">여기가 지금 우리가 만들고 있는 부분입니다.</lang>

47
00:03:44,233 --> 00:03:45,525
<lang xml:lang="ko-KR">이것이 쿼리이고,</lang>

48
00:03:46,116 --> 00:03:47,960
<lang xml:lang="ko-KR">이것이 키와 밸류입니다.</lang>

49
00:03:48,550 --> 00:03:54,358
<lang xml:lang="ko-KR">뭔가 말이 되는 것 같습니다. 프랑스어를 영어로 번역하려고 하는데, 쿼리는 다음

50
00:03:54,458 --> 00:03:59,826
영어 단어는 무엇일까를 묻는 것이고, 키와 밸류는 프랑스어 문장의 의미인 context인 것입니다.</lang>

51
00:04:01,416 --> 00:04:07,499
<lang xml:lang="ko-KR">그리고는 쿼리, 키, 밸류에 multi head attention을 적용하고, 그

52
00:04:07,599 --> 00:04:13,774
결과를 attention output라는 변수에 저장하는 것입니다. 이것이 cross attention입니다. cross라는

53
00:04:13,874 --> 00:04:17,320
말은 쿼리의 소스와 키 밸류의 소스가 서로 다르다는 의미입니다. 나중에

54
00:04:17,420 --> 00:04:22,412
살펴볼 self attention의 코드와 비교해보면 그 의미가 더욱 명확해질 것입니다.</lang>

55
00:04:23,000 --> 00:04:28,494
<lang xml:lang="ko-KR">이번에는 add and norm 부분을 구현할 것입니다. 엄청 어려울

56
00:04:28,594 --> 00:04:32,438
것 같지만, 사실은 아래의 두 문장입니다. 방금 전에 계산한 attention

57
00:04:32,538 --> 00:04:37,300
output과 쿼리에 사용되었던 x를 더하고, 그 결과를 normalize 합니다.</lang>

58
00:04:37,900 --> 00:04:40,608
<lang xml:lang="ko-KR">이번에도 그림으로 설명을 해보겠습니다.</lang>

59
00:04:41,200 --> 00:04:46,004
<lang xml:lang="ko-KR">여기가 x입니다. multi head attention에 넣었던 x와

60
00:04:46,104 --> 00:04:49,140
똑같은 x입니다. 다만 다른 위치로 향하고 있을 뿐입니다.</lang>

61
00:04:49,733 --> 00:04:52,346
<lang xml:lang="ko-KR">가운데 있던 점이 움직인 곳이

62
00:04:52,446 --> 00:04:55,129
attention output입니다. 작아서 잘 안보입니다.</lang>

63
00:04:55,716 --> 00:04:58,304
<lang xml:lang="ko-KR">암튼, 그 두가지를 서로 더하고,</lang>

64
00:04:58,900 --> 00:05:02,052
<lang xml:lang="ko-KR">그 결과에 layer normalization을

65
00:05:02,152 --> 00:05:04,176
적용하는 것입니다. 그리고 return합니다.</lang>

66
00:05:07,766 --> 00:05:11,026
<lang xml:lang="ko-KR">지금 설명한 add and norm은 모형이 더

67
00:05:11,126 --> 00:05:13,426
빨리 수렴할 수 있도록 도와주는 layer입니다. </lang>

68
00:05:14,016 --> 00:05:16,820
<lang xml:lang="ko-KR">이상으로 크로스 어텐션을 구현하였습니다. </lang>

