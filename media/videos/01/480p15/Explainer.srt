1
00:00:00,000 --> 00:00:05,973
<lang xml:lang="ko-KR">이번 시간에는 세가지의 서로 다른 어텐션을 코드로 어떻게 구현하는가를 살펴보도록

2
00:00:06,073 --> 00:00:12,500
하겠습니다. TensorFlow code를 기준으로 살펴보지만, pi torch에서도 거의 같은 방법으로 사용됩니다.</lang>

3
00:00:13,066 --> 00:00:14,526
<lang xml:lang="ko-KR">시작해보겠습니다.</lang>

4
00:00:15,066 --> 00:00:21,321
<lang xml:lang="ko-KR">우선 base attention이라는 이름을 갖는 클래스를 정의하겠습니다. 이 클래스의 목적은

5
00:00:21,421 --> 00:00:26,150
세가지의 서로 다른 어텐션에서 공통적으로 사용될 layer들을 미리 정의해 놓는 것입니다.</lang>

6
00:00:26,733 --> 00:00:31,996
<lang xml:lang="ko-KR">TensorFlow의 모든 layer는 tf keras layers layer라는 상위

7
00:00:32,096 --> 00:00:35,849
class로부터 상속을 받아야 합니다. 그냥 default라고 생각하면 됩니다.</lang>

8
00:00:36,400 --> 00:00:43,063
<lang xml:lang="ko-KR">이제 이 class의 initialization 스텝을 정의하는, init 라는 함수를

9
00:00:43,163 --> 00:00:47,064
만들겠습니다. 이 함수는 기본적으로 두가지 종류의 일을 합니다. 첫째는 나의

10
00:00:47,164 --> 00:00:51,732
상위클래스를 init하고, 둘째는 내가 사용할 변수를 지정해 놓는 것입니다.</lang>

11
00:00:52,266 --> 00:00:58,287
<lang xml:lang="ko-KR">이 문장은 init 함수의 첫번째 역할인 나의 상위클래스를 init 하는

12
00:00:58,387 --> 00:01:04,213
것입니다. super는 나의 상위 클래스를 의미합니다. 즉, default라고 설명했던 keras layer를

13
00:01:04,313 --> 00:01:08,390
초기화하는 것입니다. 이 문장은 거의 항상 이렇게만 사용됩니다. </lang>

14
00:01:08,933 --> 00:01:14,833
<lang xml:lang="ko-KR">이번에는 init 함수의 두번째 역할인, 사용될 변수들을 정의하는

15
00:01:14,933 --> 00:01:19,205
과정입니다. 세개의 변수를 정의하겠습니다. 이 세가지 변수만 정의하면 base

16
00:01:19,305 --> 00:01:24,697
attention은 끝입니다. 일단, 이름을 보고 무엇을 만들지 짐작해보기 바랍니다.</lang>

17
00:01:26,266 --> 00:01:33,413
<lang xml:lang="ko-KR">m h a는 Multi head attention을 의미합니다. 다행히도 TensorFlow에는 이 layer가

18
00:01:33,513 --> 00:01:37,590
이미 만들어져 있습니다. 그냥 불러서 m h a라는 변수에 넣어주면 됩니다.</lang>

19
00:01:38,133 --> 00:01:44,638
<lang xml:lang="ko-KR">다음으로는 layer normalization이라는 layer를 부르고 그것을 layernorm에

20
00:01:44,738 --> 00:01:47,983
넣습니다. 이 layer의 구체적인 역할에 대해서는 다시 설명하겠지만,

21
00:01:48,083 --> 00:01:51,329
기본적으로 모형이 더 빠르게 수렴하게 하는 역할을 합니다.</lang>

22
00:01:51,866 --> 00:01:58,370
<lang xml:lang="ko-KR">마지막 layer는 더하기 layer입니다. Transformer 아키텍쳐를 기억해 보면, Add and

23
00:01:58,470 --> 00:02:02,854
Norm 이라는 부분이 있었습니다. 그 부분을 구현하기 위해서 필요한 layer입니다.</lang>

24
00:02:03,400 --> 00:02:07,641
<lang xml:lang="ko-KR">지금까지 만든 모든 layer는 다 keras layer입니다.

25
00:02:07,741 --> 00:02:10,860
따라서 앞에 tf keras layers 라고 붙여주어야 합니다.</lang>

26
00:02:11,400 --> 00:02:17,532
<lang xml:lang="ko-KR">마지막으로 이 부분. keyword arguments 부분이 굉장히 신경쓰일텐데, 이것은

27
00:02:17,632 --> 00:02:21,428
나중에 이 클래스를 부를 때 보내주는 인자를 받아오기 위한 것입니다.</lang>

28
00:02:22,000 --> 00:02:26,628
<lang xml:lang="ko-KR">예를 들어, 이렇게 인자들을 보내면, 이것을 여기에서 받아옵니다.</lang>

29
00:02:26,733 --> 00:02:29,801
<lang xml:lang="ko-KR">그리고는, 여기에서 여기로 들어가게 됩니다.</lang>

30
00:02:30,933 --> 00:02:35,470
<lang xml:lang="ko-KR">이상으로 base attention class를 만들어 보았습니다. 이것을

31
00:02:35,570 --> 00:02:38,585
기반으로 세가지의 서로 다른 attention을 구현할 것입니다.</lang>

