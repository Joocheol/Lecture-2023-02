1
00:00:00,000 --> 00:00:04,130
<lang xml:lang="ko-KR"> 이번 시간에는 세가지의 서로 다른 어텐션을

2
00:00:04,230 --> 00:00:08,540
코드로 어떻게 구현하는가를 살펴보도록 하겠습니다. TensorFlow code를 기준으로

3
00:00:08,640 --> 00:00:12,500
살펴보지만, pi torch에서도 거의 같은 방법으로 사용됩니다. </lang>

4
00:00:13,600 --> 00:00:15,060
<lang xml:lang="ko-KR"> 시작해보겠습니다. </lang>

5
00:00:15,133 --> 00:00:21,376
<lang xml:lang="ko-KR"> 우선 base attention이라는 이름을 갖는 클래스를 정의하겠습니다. 이 클래스의 목적은

6
00:00:21,476 --> 00:00:26,217
세가지의 서로 다른 어텐션에서 공통적으로 사용될 layer들을 미리 정의해 놓는 것입니다. </lang>

7
00:00:26,266 --> 00:00:31,517
<lang xml:lang="ko-KR"> TensorFlow의 모든 layer는 tf keras layers layer라는 상위

8
00:00:31,617 --> 00:00:35,382
class로부터 상속을 받아야 합니다. 그냥 default라고 생각하면 됩니다. </lang>

9
00:00:35,466 --> 00:00:38,707
<lang xml:lang="ko-KR"> 이제 base attention class가 만들어질

10
00:00:38,807 --> 00:00:40,598
때 초기화해야 하는 일을 정의하겠습니다. </lang>

11
00:00:40,666 --> 00:00:46,149
<lang xml:lang="ko-KR"> super는 나의 상위 클래스를 의미합니다. 즉, default라고 설명했던

12
00:00:46,249 --> 00:00:50,886
keras layer를 초기화하는 것입니다. 이것도 default라고 생각하면 됩니다. </lang>

13
00:00:50,933 --> 00:00:56,548
<lang xml:lang="ko-KR"> 이제 세개의 변수를 정의하겠습니다. 이 세가지 변수만 정의하면 base

14
00:00:56,648 --> 00:01:01,537
attention은 끝입니다. 일단, 이름을 보고 무엇을 만들지 짐작해보기 바랍니다. </lang>

15
00:01:04,600 --> 00:01:08,361
<lang xml:lang="ko-KR"> Multi head attention이라는 layer를 부르고,

16
00:01:08,461 --> 00:01:10,260
그것을 m h a라는 변수에 넣습니다. </lang>

17
00:01:10,333 --> 00:01:16,838
<lang xml:lang="ko-KR"> 다음으로는 layer normalization이라는 layer를 부르고 그것을 layernorm에

18
00:01:16,938 --> 00:01:20,649
넣습니다. 이 layer의 구체적인 역할에 대해서는 다시 설명하겠지만, 기본적으로

19
00:01:20,749 --> 00:01:23,529
모형이 더 빠르게 수렴하게 하는 역할을 합니다. </lang>

20
00:01:23,600 --> 00:01:30,088
<lang xml:lang="ko-KR"> 마지막 layer는 더하기 layer입니다. Transformer 아키텍쳐를 기억해 보면, Add and

21
00:01:30,188 --> 00:01:34,588
Norm 이라는 부분이 있었습니다. 그 부분을 구현하기 위해서 필요한 layer입니다. </lang>

22
00:01:34,666 --> 00:01:38,897
<lang xml:lang="ko-KR"> 지금까지 만든 모든 layer는 다 keras layer입니다.

23
00:01:38,997 --> 00:01:42,126
따라서 앞에 tf keras layers 라고 붙여주어야 합니다. </lang>

24
00:01:43,200 --> 00:01:49,312
<lang xml:lang="ko-KR"> 마지막으로 이 부분. keyword arguments 부분이 굉장히 신경쓰일텐데, 이것은

25
00:01:49,412 --> 00:01:53,228
나중에 이 클래스를 부를 때 보내주는 인자를 받아오기 위한 것입니다. </lang>

26
00:01:54,266 --> 00:01:58,894
<lang xml:lang="ko-KR"> 예를 들어, 이렇게 인자들을 보내면, 이것을 여기에서 받아옵니다. </lang>

27
00:02:00,000 --> 00:02:03,346
<lang xml:lang="ko-KR"> 이상의 base attention class를 기반으로

28
00:02:03,446 --> 00:02:05,900
세가지의 서로 다른 attention을 구현해 보겠습니다. </lang>

29
00:02:08,000 --> 00:02:12,342
<lang xml:lang="ko-KR"> 이번에는 cross attention 부분을 어떻게 구현하는가를

30
00:02:12,442 --> 00:02:16,108
살펴보겠습니다. 앞에서 만들었던 base attetion을 이용할 것입니다. </lang>

31
00:02:18,200 --> 00:02:19,660
<lang xml:lang="ko-KR"> 시작해보겠습니다. </lang>

32
00:02:20,733 --> 00:02:23,441
<lang xml:lang="ko-KR"> 이 클래스의 이름은 cross attention입니다. </lang>

33
00:02:24,533 --> 00:02:30,163
<lang xml:lang="ko-KR"> 이 클래스는 앞에서 만들었던 base attention을 상속받습니다. 따라서 굳이

34
00:02:30,263 --> 00:02:33,601
m h a, add 등을 설정하지 않아도 사용할 수 있습니다. </lang>

35
00:02:33,666 --> 00:02:36,907
<lang xml:lang="ko-KR"> 이제 base attention class가 만들어질

36
00:02:37,007 --> 00:02:38,798
때 초기화해야 하는 일을 정의하겠습니다. </lang>

37
00:02:38,866 --> 00:02:44,349
<lang xml:lang="ko-KR"> super는 나의 상위 클래스를 의미합니다. 즉, default라고 설명했던

38
00:02:44,449 --> 00:02:49,086
keras layer를 초기화하는 것입니다. 이것도 default라고 생각하면 됩니다. </lang>

39
00:02:49,133 --> 00:02:54,748
<lang xml:lang="ko-KR"> 이제 세개의 변수를 정의하겠습니다. 이 세가지 변수만 정의하면 base

40
00:02:54,848 --> 00:02:59,737
attention은 끝입니다. 일단, 이름을 보고 무엇을 만들지 짐작해보기 바랍니다. </lang>

41
00:03:02,800 --> 00:03:06,561
<lang xml:lang="ko-KR"> Multi head attention이라는 layer를 부르고,

42
00:03:06,661 --> 00:03:08,460
그것을 m h a라는 변수에 넣습니다. </lang>

43
00:03:08,533 --> 00:03:15,038
<lang xml:lang="ko-KR"> 다음으로는 layer normalization이라는 layer를 부르고 그것을 layernorm에

44
00:03:15,138 --> 00:03:18,849
넣습니다. 이 layer의 구체적인 역할에 대해서는 다시 설명하겠지만, 기본적으로

45
00:03:18,949 --> 00:03:21,729
모형이 더 빠르게 수렴하게 하는 역할을 합니다. </lang>

46
00:03:21,800 --> 00:03:28,288
<lang xml:lang="ko-KR"> 마지막 layer는 더하기 layer입니다. Transformer 아키텍쳐를 기억해 보면, Add and

47
00:03:28,388 --> 00:03:32,788
Norm 이라는 부분이 있었습니다. 그 부분을 구현하기 위해서 필요한 layer입니다. </lang>

48
00:03:32,866 --> 00:03:37,097
<lang xml:lang="ko-KR"> 지금까지 만든 모든 layer는 다 keras layer입니다.

49
00:03:37,197 --> 00:03:40,326
따라서 앞에 tf keras layers 라고 붙여주어야 합니다. </lang>

50
00:03:41,400 --> 00:03:47,512
<lang xml:lang="ko-KR"> 마지막으로 이 부분. keyword arguments 부분이 굉장히 신경쓰일텐데, 이것은

51
00:03:47,612 --> 00:03:51,428
나중에 이 클래스를 부를 때 보내주는 인자를 받아오기 위한 것입니다. </lang>

52
00:03:52,466 --> 00:03:57,094
<lang xml:lang="ko-KR"> 예를 들어, 이렇게 인자들을 보내면, 이것을 여기에서 받아옵니다. </lang>

53
00:03:58,200 --> 00:04:01,546
<lang xml:lang="ko-KR"> 이상의 base attention class를 기반으로

54
00:04:01,646 --> 00:04:04,100
세가지의 서로 다른 attention을 구현해 보겠습니다. </lang>

