1
00:00:00,000 --> 00:00:05,973
<lang xml:lang="ko-KR">이번 시간에는 세가지의 서로 다른 어텐션을 코드로 어떻게 구현하는가를 살펴보도록

2
00:00:06,073 --> 00:00:12,500
하겠습니다. TensorFlow code를 기준으로 살펴보지만, pi torch에서도 거의 같은 방법으로 사용됩니다.</lang>

3
00:00:13,066 --> 00:00:14,526
<lang xml:lang="ko-KR">시작해보겠습니다.</lang>

4
00:00:15,066 --> 00:00:21,321
<lang xml:lang="ko-KR">우선 base attention이라는 이름을 갖는 클래스를 정의하겠습니다. 이 클래스의 목적은

5
00:00:21,421 --> 00:00:26,150
세가지의 서로 다른 어텐션에서 공통적으로 사용될 layer들을 미리 정의해 놓는 것입니다.</lang>

6
00:00:26,733 --> 00:00:31,996
<lang xml:lang="ko-KR">TensorFlow의 모든 layer는 tf keras layers layer라는 상위

7
00:00:32,096 --> 00:00:35,849
class로부터 상속을 받아야 합니다. 그냥 default라고 생각하면 됩니다.</lang>

8
00:00:36,400 --> 00:00:43,063
<lang xml:lang="ko-KR">이제 이 class의 initialization 스텝을 정의하는, init 라는 함수를

9
00:00:43,163 --> 00:00:47,064
만들겠습니다. 이 함수는 기본적으로 두가지 종류의 일을 합니다. 첫째는 나의

10
00:00:47,164 --> 00:00:51,732
상위클래스를 init하고, 둘째는 내가 사용할 변수를 지정해 놓는 것입니다.</lang>

11
00:00:52,266 --> 00:00:58,287
<lang xml:lang="ko-KR">이 문장은 init 함수의 첫번째 역할인 나의 상위클래스를 init 하는

12
00:00:58,387 --> 00:01:04,213
것입니다. super는 나의 상위 클래스를 의미합니다. 즉, default라고 설명했던 keras layer를

13
00:01:04,313 --> 00:01:08,390
초기화하는 것입니다. 이 문장은 거의 항상 이렇게만 사용됩니다. </lang>

14
00:01:08,933 --> 00:01:14,833
<lang xml:lang="ko-KR">이번에는 init 함수의 두번째 역할인, 사용될 변수들을 정의하는

15
00:01:14,933 --> 00:01:19,205
과정입니다. 세개의 변수를 정의하겠습니다. 이 세가지 변수만 정의하면 base

16
00:01:19,305 --> 00:01:24,697
attention은 끝입니다. 일단, 이름을 보고 무엇을 만들지 짐작해보기 바랍니다.</lang>

17
00:01:26,266 --> 00:01:33,413
<lang xml:lang="ko-KR">m h a는 Multi head attention을 의미합니다. 다행히도 TensorFlow에는 이 layer가

18
00:01:33,513 --> 00:01:37,590
이미 만들어져 있습니다. 그냥 불러서 m h a라는 변수에 넣어주면 됩니다.</lang>

19
00:01:38,133 --> 00:01:44,638
<lang xml:lang="ko-KR">다음으로는 layer normalization이라는 layer를 부르고 그것을 layernorm에

20
00:01:44,738 --> 00:01:47,983
넣습니다. 이 layer의 구체적인 역할에 대해서는 다시 설명하겠지만,

21
00:01:48,083 --> 00:01:51,329
기본적으로 모형이 더 빠르게 수렴하게 하는 역할을 합니다.</lang>

22
00:01:51,866 --> 00:01:58,370
<lang xml:lang="ko-KR">마지막 layer는 더하기 layer입니다. Transformer 아키텍쳐를 기억해 보면, Add and

23
00:01:58,470 --> 00:02:02,854
Norm 이라는 부분이 있었습니다. 그 부분을 구현하기 위해서 필요한 layer입니다.</lang>

24
00:02:03,400 --> 00:02:07,641
<lang xml:lang="ko-KR">지금까지 만든 모든 layer는 다 keras layer입니다.

25
00:02:07,741 --> 00:02:10,860
따라서 앞에 tf keras layers 라고 붙여주어야 합니다.</lang>

26
00:02:11,400 --> 00:02:17,532
<lang xml:lang="ko-KR">마지막으로 이 부분. keyword arguments 부분이 굉장히 신경쓰일텐데, 이것은

27
00:02:17,632 --> 00:02:21,428
나중에 이 클래스를 부를 때 보내주는 인자를 받아오기 위한 것입니다.</lang>

28
00:02:22,000 --> 00:02:26,628
<lang xml:lang="ko-KR">예를 들어, 이렇게 인자들을 보내면, 이것을 여기에서 받아옵니다.</lang>

29
00:02:26,733 --> 00:02:29,801
<lang xml:lang="ko-KR">그리고는, 여기에서 여기로 들어가게 됩니다.</lang>

30
00:02:30,933 --> 00:02:35,470
<lang xml:lang="ko-KR">이상으로 base attention class를 만들어 보았습니다. 이것을

31
00:02:35,570 --> 00:02:38,585
기반으로 세가지의 서로 다른 attention을 구현할 것입니다.</lang>

32
00:02:41,933 --> 00:02:46,241
<lang xml:lang="ko-KR">이번에는 cross attention 부분을 어떻게 구현하는가를

33
00:02:46,341 --> 00:02:50,041
살펴보겠습니다. 앞에서 만들었던 base attention을 이용할 것입니다.</lang>

34
00:02:50,600 --> 00:02:52,060
<lang xml:lang="ko-KR">시작해보겠습니다.</lang>

35
00:02:52,600 --> 00:02:56,416
<lang xml:lang="ko-KR">이 클래스의 이름은 cross attention입니다. 왜

36
00:02:56,516 --> 00:02:58,980
cross라는 말을 사용하는지 곧 알게 될 것입니다.</lang>

37
00:02:59,533 --> 00:03:05,243
<lang xml:lang="ko-KR">이 클래스는 앞에서 만들었던 base attention을 상속받습니다. 따라서 앞에서 정의했던,

38
00:03:05,343 --> 00:03:09,753
multi head attention, layernorm, add 등을 사용할 수 있습니다. </lang>

39
00:03:10,333 --> 00:03:15,524
<lang xml:lang="ko-KR">TensorFlow에서는 class가 실행할 일을, call 이라는 함수를 이용하여

40
00:03:15,624 --> 00:03:19,281
만듭니다. pi torch에서는 forward 라는 함수 이름을 사용합니다.</lang>

41
00:03:19,866 --> 00:03:23,428
<lang xml:lang="ko-KR">함수를 부를 때, 사용하는 인수는 x 와

42
00:03:23,528 --> 00:03:26,846
context입니다. self는 그냥 원래 적어주는 것입니다. </lang>

43
00:03:27,400 --> 00:03:31,489
<lang xml:lang="ko-KR">쿼리, 키, 밸류라는 말이 나옵니다. 쿼리에는

44
00:03:31,589 --> 00:03:34,980
x를 넣어주고, 키와 밸류에는 context를 넣어줍니다. </lang>

45
00:03:35,533 --> 00:03:39,513
<lang xml:lang="ko-KR">이것을 트랜스포머 아키텍쳐를 보면서 설명해보겠습니다.</lang>

46
00:03:40,066 --> 00:03:42,966
<lang xml:lang="ko-KR">여기가 지금 우리가 만들고 있는 부분입니다.</lang>

47
00:03:43,533 --> 00:03:44,825
<lang xml:lang="ko-KR">이것이 쿼리이고,</lang>

48
00:03:45,400 --> 00:03:47,244
<lang xml:lang="ko-KR">이것이 키와 밸류입니다.</lang>

49
00:03:47,800 --> 00:03:53,608
<lang xml:lang="ko-KR">뭔가 말이 되는 것 같습니다. 프랑스어를 영어로 번역하려고 하는데, 쿼리는 다음

50
00:03:53,708 --> 00:03:59,076
영어 단어는 무엇일까를 묻는 것이고, 키와 밸류는 프랑스어 문장의 의미인 context인 것입니다.</lang>

51
00:04:00,666 --> 00:04:06,749
<lang xml:lang="ko-KR">그리고는 쿼리, 키, 밸류에 multi head attention을 적용하고, 그

52
00:04:06,849 --> 00:04:13,024
결과를 attention output라는 변수에 저장하는 것입니다. 이것이 cross attention입니다. cross라는

53
00:04:13,124 --> 00:04:16,570
말은 쿼리의 소스와 키 밸류의 소스가 서로 다르다는 의미입니다. 나중에

54
00:04:16,670 --> 00:04:21,662
살펴볼 self attention의 코드와 비교해보면 그 의미가 더욱 명확해질 것입니다.</lang>

55
00:04:22,200 --> 00:04:27,694
<lang xml:lang="ko-KR">이번에는 add and norm 부분을 구현할 것입니다. 엄청 어려울

56
00:04:27,794 --> 00:04:31,638
것 같지만, 사실은 아래의 두 문장입니다. 방금 전에 계산한 attention

57
00:04:31,738 --> 00:04:36,500
output과 쿼리에 사용되었던 x를 더하고, 그 결과를 normalize 합니다.</lang>

58
00:04:37,066 --> 00:04:39,774
<lang xml:lang="ko-KR">이번에도 그림으로 설명을 해보겠습니다.</lang>

59
00:04:40,333 --> 00:04:45,137
<lang xml:lang="ko-KR">여기가 x입니다. multi head attention에 넣었던 x와

60
00:04:45,237 --> 00:04:48,273
똑같은 x입니다. 다만 다른 위치로 향하고 있을 뿐입니다.</lang>

61
00:04:48,866 --> 00:04:51,479
<lang xml:lang="ko-KR">가운데 있던 점이 움직인 곳이

62
00:04:51,579 --> 00:04:54,262
attention output입니다. 작아서 잘 안보입니다.</lang>

63
00:04:54,800 --> 00:04:57,388
<lang xml:lang="ko-KR">암튼, 그 두가지를 서로 더하고,</lang>

64
00:04:57,933 --> 00:05:01,085
<lang xml:lang="ko-KR">그 결과에 layer normalization을

65
00:05:01,185 --> 00:05:03,209
적용하는 것입니다. 그리고 return합니다.</lang>

66
00:05:06,800 --> 00:05:10,060
<lang xml:lang="ko-KR">지금 설명한 add and norm은 모형이 더

67
00:05:10,160 --> 00:05:12,460
빨리 수렴할 수 있도록 도와주는 layer입니다. </lang>

68
00:05:13,000 --> 00:05:15,804
<lang xml:lang="ko-KR">이상으로 크로스 어텐션을 구현하였습니다. </lang>

