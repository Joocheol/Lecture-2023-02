이것이 트랜스포머 아키텍쳐입니다.
보기만 해도 어지럽습니다.
우리의 목표는 여기에 보이는 모든 부분을 하나도 빠짐없이 모두 이해하는 것입니다.


이번 시간에는 세가지의 서로 다른 어텐션을 코드로 어떻게 구현하는가를 살펴보도록 하겠습니다. TensorFlow code를 기준으로 살펴보지만, pi torch에서도 거의 같은 방법으로 사용됩니다.
시작해보겠습니다.
우선 base attention이라는 이름을 갖는 클래스를 정의하겠습니다. 이 클래스의 목적은 세가지의 서로 다른 어텐션에서 공통적으로 사용될 layer들을 미리 정의해 놓는 것입니다.
TensorFlow의 모든 layer는 tf keras layers layer라는 상위 class로부터 상속을 받아야 합니다. 그냥 default라고 생각하면 됩니다.
이제 이 class의 initialization 스텝을 정의하는, init 라는 함수를 만들겠습니다. 이 함수는 기본적으로 두가지 종류의 일을 합니다. 첫째는 나의 상위클래스를 init하고, 둘째는 내가 사용할 변수를 지정해 놓는 것입니다.
이 문장은 init 함수의 첫번째 역할인 나의 상위클래스를 init 하는 것입니다. super는 나의 상위 클래스를 의미합니다. 즉, default라고 설명했던 keras layer를 초기화하는 것입니다. 이 문장은 거의 항상 이렇게만 사용됩니다. 
이번에는 init 함수의 두번째 역할인, 사용될 변수들을 정의하는 과정입니다.  세개의 변수를 정의하겠습니다. 이 세가지 변수만 정의하면 base attention은 끝입니다. 일단, 이름을 보고 무엇을 만들지 짐작해보기 바랍니다.
m h a는 Multi head attention을 의미합니다. 다행히도 TensorFlow에는 이 layer가 이미 만들어져 있습니다. 그냥 불러서 m h a라는 변수에 넣어주면 됩니다.
다음으로는 layer normalization이라는 layer를 부르고 그것을 layernorm에 넣습니다. 이 layer의 구체적인 역할에 대해서는 다시 설명하겠지만, 기본적으로 모형이 더 빠르게 수렴하게 하는 역할을 합니다.
마지막 layer는 더하기 layer입니다. Transformer 아키텍쳐를 기억해 보면, Add and Norm 이라는 부분이 있었습니다. 그 부분을 구현하기 위해서 필요한 layer입니다.
지금까지 만든 모든 layer는 다 keras layer입니다. 따라서 앞에 tf keras layers 라고 붙여주어야 합니다.
마지막으로 이 부분. keyword arguments 부분이 굉장히 신경쓰일텐데, 이것은 나중에 이 클래스를 부를 때 보내주는 인자를 받아오기 위한 것입니다.
예를 들어, 이렇게 인자들을 보내면, 이것을 여기에서 받아옵니다.
그리고는, 여기에서 여기로 들어가게 됩니다.
이상으로 base attention class를 만들어 보았습니다. 이것을 기반으로 세가지의 서로 다른 attention을 구현할 것입니다.

이번에는 cross attention 부분을 어떻게 구현하는가를 살펴보겠습니다. 앞에서 만들었던 base attetion을 이용할 것입니다.
시작해보겠습니다.
이 클래스의 이름은 cross attention입니다. 
이 클래스는 앞에서 만들었던 base attention을 상속받습니다.
따라서 굳이 m h a, add 등을 설정하지 않아도 사용할 수 있습니다.
이제 base attention class가 만들어질 때 초기화해야 하는 일을 정의하겠습니다.
super는 나의 상위 클래스를 의미합니다. 즉, default라고 설명했던 keras layer를 초기화하는 것입니다. 이것도 default라고 생각하면 됩니다.
이제 세개의 변수를 정의하겠습니다. 이 세가지 변수만 정의하면 base attention은 끝입니다. 일단, 이름을 보고 무엇을 만들지 짐작해보기 바랍니다.
Multi head attention이라는 layer를 부르고, 그것을 m h a라는 변수에 넣습니다.
다음으로는 layer normalization이라는 layer를 부르고 그것을 layernorm에 넣습니다. 이 layer의 구체적인 역할에 대해서는 다시 설명하겠지만, 기본적으로 모형이 더 빠르게 수렴하게 하는 역할을 합니다.
마지막 layer는 더하기 layer입니다. Transformer 아키텍쳐를 기억해 보면, Add and Norm 이라는 부분이 있었습니다. 그 부분을 구현하기 위해서 필요한 layer입니다.
지금까지 만든 모든 layer는 다 keras layer입니다. 따라서 앞에 tf keras layers 라고 붙여주어야 합니다.
마지막으로 이 부분. keyword arguments 부분이 굉장히 신경쓰일텐데, 이것은 나중에 이 클래스를 부를 때 보내주는 인자를 받아오기 위한 것입니다.
예를 들어, 이렇게 인자들을 보내면, 이것을 여기에서 받아옵니다.
이상으로 base attention class를 기반으로 세가지의 서로 다른 attention을 구현해 보겠습니다.


